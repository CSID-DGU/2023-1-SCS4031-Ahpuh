{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-cu112\n",
      "  Downloading mxnet_cu112-1.9.1-py3-none-manylinux2014_x86_64.whl (499.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 499.4 MB 794 bytes/s a 0:00:011\n",
      "\u001b[?25hCollecting graphviz<0.9.0,>=0.8.1\n",
      "  Using cached graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/irteam/anaconda3/lib/python3.7/site-packages (from mxnet-cu112) (2.22.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/irteam/anaconda3/lib/python3.7/site-packages (from mxnet-cu112) (1.18.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu112) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu112) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu112) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu112) (3.0.4)\n",
      "Installing collected packages: graphviz, mxnet-cu112\n",
      "Successfully installed graphviz-0.8.4 mxnet-cu112-1.9.1\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  libnccl2\n",
      "0 upgraded, 1 newly installed, 0 to remove and 177 not upgraded.\n",
      "Need to get 97.5 MB of archives.\n",
      "After this operation, 268 MB of additional disk space will be used.\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libnccl2 2.15.5-1+cuda11.8 [97.5 MB]\n",
      "Fetched 97.5 MB in 5s (18.2 MB/s)    \u001b[0m3m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libnccl2.\n",
      "(Reading database ... 75037 files and directories currently installed.)\n",
      "Preparing to unpack .../libnccl2_2.15.5-1+cuda11.8_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 17%]\u001b[49m\u001b[39m [#########.................................................] \u001b8Unpacking libnccl2 (2.15.5-1+cuda11.8) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [###################.......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up libnccl2 (2.15.5-1+cuda11.8) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 67%]\u001b[49m\u001b[39m [######################################....................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [################################################..........] \u001b8Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
      "\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JW: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-chrome.list:3 and /etc/apt/sources.list.d/google.list:1\n",
      "W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-chrome.list:3 and /etc/apt/sources.list.d/google.list:1\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet-cu112\n",
    "!sudo apt install libnccl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gluoncv\n",
      "  Using cached gluoncv-0.10.5.post0-py2.py3-none-any.whl (1.3 MB)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (1.18.1)\n",
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "Collecting portalocker\n",
      "  Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (4.42.1)\n",
      "Requirement already satisfied, skipping upgrade: Pillow in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (7.0.0)\n",
      "Collecting yacs\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (5.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (1.4.1)\n",
      "Collecting autocfg\n",
      "  Using cached autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /home/irteam/anaconda3/lib/python3.7/site-packages (from gluoncv) (3.1.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests->gluoncv) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests->gluoncv) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests->gluoncv) (1.25.8)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests->gluoncv) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /home/irteam/anaconda3/lib/python3.7/site-packages (from pandas->gluoncv) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /home/irteam/anaconda3/lib/python3.7/site-packages (from pandas->gluoncv) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/irteam/anaconda3/lib/python3.7/site-packages (from matplotlib->gluoncv) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/irteam/anaconda3/lib/python3.7/site-packages (from matplotlib->gluoncv) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/irteam/anaconda3/lib/python3.7/site-packages (from matplotlib->gluoncv) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /home/irteam/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->gluoncv) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/irteam/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->gluoncv) (45.2.0.post20200210)\n",
      "Installing collected packages: opencv-python, portalocker, yacs, autocfg, gluoncv\n",
      "Successfully installed autocfg-0.0.8 gluoncv-0.10.5.post0 opencv-python-4.7.0.72 portalocker-2.7.0 yacs-0.1.8\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gluoncv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Dive Deep into Training SlowFast mdoels on Kinetcis400\n",
    "\n",
    "This is a video action recognition tutorial using Gluon CV toolkit, a step-by-step example.\n",
    "The readers should have basic knowledge of deep learning and should be familiar with Gluon API.\n",
    "New users may first go through `A 60-minute Gluon Crash Course <http://gluon-crash-course.mxnet.io/>`_.\n",
    "You can `Start Training Now`_ or `Dive into Deep`_.\n",
    "\n",
    "## Start Training Now\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Feel free to skip the tutorial because the training script is self-complete and ready to launch.\n",
    "\n",
    "    :download:`Download Full Python Script: train_recognizer.py<../../../scripts/action-recognition/train_recognizer.py>`\n",
    "\n",
    "    For more training command options, please run ``python train_recognizer.py -h``\n",
    "    Please checkout the `model_zoo <../model_zoo/index.html#action_recognition>`_ for training commands of reproducing the pretrained model.</p></div>\n",
    "\n",
    "\n",
    "### Network Structure\n",
    "\n",
    "First, let's import the necessary libraries into python.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mxnet-cu101\n",
      "  Downloading mxnet_cu101-1.9.1-py3-none-manylinux2014_x86_64.whl (360.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 360.0 MB 29 kB/s /s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: graphviz<0.9.0,>=0.8.1 in /home/irteam/anaconda3/lib/python3.7/site-packages (from mxnet-cu101) (0.8.4)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/irteam/anaconda3/lib/python3.7/site-packages (from mxnet-cu101) (1.18.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /home/irteam/anaconda3/lib/python3.7/site-packages (from mxnet-cu101) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/irteam/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet-cu101) (1.25.8)\n",
      "Installing collected packages: mxnet-cu101\n",
      "Successfully installed mxnet-cu101-1.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install mxnet-cu101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import argparse, time, logging, os, sys, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluoncv as gcv\n",
    "from mxnet import gluon, nd, init, context\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "\n",
    "from gluoncv.data.transforms import video\n",
    "\n",
    "from gluoncv.data import Kinetics400\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, LRSequential, LRScheduler, split_and_load, TrainingHistory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pick a widely adopted model, ``SlowFast``, for the tutorial.\n",
    "`SlowFast <https://arxiv.org/abs/1812.03982>`_ is a new 3D video\n",
    "classification model, aiming for best trade-off between accuracy and efficiency.\n",
    "It proposes two branches, fast branch and slow branch, to handle different aspects in a video.\n",
    "Fast branch is to capture motion dynamics by using many but small video frames.\n",
    "Slow branch is to capture fine apperance details by using few but large video frames.\n",
    "Features from two branches are combined using lateral connections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "\n",
    "# Get the model slowfast_4x16_resnet50_kinetics400 with 400 output classes, without pre-trained weights\n",
    "net = get_model(name='slowfast_4x16_resnet50_kinetics400', nclass=4)\n",
    "net.collect_params().reset_ctx(ctx)\n",
    "print(net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation and Data Loader\n",
    "\n",
    "Data augmentation for video is different from image. For example, if you\n",
    "want to randomly crop a video sequence, you need to make sure all the video\n",
    "frames in this sequence undergo the same cropping process. We provide a\n",
    "new set of transformation functions, working with multiple images.\n",
    "Please checkout the `video.py <../../../gluoncv/data/transforms/video.py>`_ for more details.\n",
    "Most video data augmentation strategies used here are introduced in [Wang15]_.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e6e6aa7cb624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform_train = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Fix the input video frames size as 256×340 and randomly sample the cropping width and height from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# {256,224,192,168}. After that, resize the cropped regions to 224 × 224.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Randomly flip the video frames horizontally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    # Fix the input video frames size as 256×340 and randomly sample the cropping width and height from\n",
    "    # {256,224,192,168}. After that, resize the cropped regions to 224 × 224.\n",
    "    video.VideoCenterCrop(size=(224, 224)),\n",
    "    # Randomly flip the video frames horizontally\n",
    "    video.VideoRandomHorizontalFlip(),\n",
    "    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    video.VideoToTensor(),\n",
    "    # Normalize the video frames with mean and standard deviation calculated across all images\n",
    "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the transform functions, we can define data loaders for our\n",
    "training datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load 185 training samples.\n"
     ]
    }
   ],
   "source": [
    "# Batch Size for Each GP\n",
    "per_device_batch_size = 10\n",
    "# Number of data loader workers\n",
    "num_workers = 0\n",
    "# Calculate effective total batch size\n",
    "batch_size = per_device_batch_size * num_gpus\n",
    "\n",
    "\n",
    "# Set train=True for training the model.\n",
    "# ``new_length`` indicates the number of frames we will cover.\n",
    "# For SlowFast network, we evenly sample 32 frames for the fast branch and 4 frames for the slow branch.\n",
    "# This leads to the actual input length of 36 video frames.\n",
    "train_dataset = Kinetics400(train=True, new_length=64, slowfast=True, transform=transform_train, root='/home/irteam/dcloud-global-dir/Ahpuh/swim_dataset/rawframes_train', setting='/home/irteam/dcloud-global-dir/Ahpuh/swim_dataset/swim_train_list_rawframes.txt')\n",
    "print('Load %d training samples.' % len(train_dataset))\n",
    "train_data = gluon.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                   shuffle=True, num_workers=num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer, Loss and Metric\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lr_decay = 0.1\n",
    "warmup_epoch = 34\n",
    "total_epoch = 196\n",
    "num_batches = len(train_data)\n",
    "lr_scheduler = LRSequential([\n",
    "    LRScheduler('linear', base_lr=0.01, target_lr=0.1,\n",
    "                nepochs=warmup_epoch, iters_per_epoch=num_batches),\n",
    "    LRScheduler('cosine', base_lr=0.1, target_lr=0,\n",
    "                nepochs=total_epoch - warmup_epoch,\n",
    "                iters_per_epoch=num_batches,\n",
    "                step_factor=lr_decay, power=2)\n",
    "])\n",
    "\n",
    "# Stochastic gradient descent\n",
    "optimizer = 'sgd'\n",
    "# Set parameters\n",
    "optimizer_params = {'learning_rate': 0.01, 'wd': 0.0001, 'momentum': 0.9}\n",
    "optimizer_params['lr_scheduler'] = lr_scheduler\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize our model, we need a loss function.\n",
    "For classification tasks, we usually use softmax cross entropy as the\n",
    "loss function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use accuracy as the metric to monitor our training\n",
    "process. Besides, we record metric values, and will print them at the\n",
    "end of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_metric = mx.metric.Accuracy()\n",
    "train_history = TrainingHistory(['training-acc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "After all the preparations, we can finally start training!\n",
    "Following is the script.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In order to finish the tutorial quickly, we only train for 0 epoch on a tiny subset of Kinetics400,\n",
    "  and 100 iterations per epoch. In your experiments, we recommend setting ``epochs=100`` for the full Kinetics400 dataset.</p></div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] train=0.513514 loss=1.230537 time: 104.334068\n",
      "[Epoch 1] train=0.524324 loss=1.547367 time: 85.579333\n",
      "[Epoch 2] train=0.513514 loss=3.150768 time: 82.320675\n",
      "[Epoch 3] train=0.518919 loss=3.554893 time: 82.714502\n",
      "[Epoch 4] train=0.524324 loss=3.568263 time: 85.405434\n",
      "[Epoch 5] train=0.529730 loss=1.863569 time: 83.962964\n",
      "[Epoch 6] train=0.540541 loss=2.164887 time: 82.732981\n",
      "[Epoch 7] train=0.524324 loss=2.897637 time: 81.822337\n",
      "[Epoch 8] train=0.572973 loss=2.644553 time: 83.196638\n",
      "[Epoch 9] train=0.578378 loss=1.702112 time: 81.053108\n",
      "[Epoch 10] train=0.648649 loss=1.144696 time: 70.741769\n",
      "[Epoch 11] train=0.637838 loss=1.272942 time: 80.186999\n",
      "[Epoch 12] train=0.600000 loss=1.379443 time: 76.679555\n",
      "[Epoch 13] train=0.627027 loss=0.955011 time: 71.060725\n",
      "[Epoch 14] train=0.648649 loss=1.097545 time: 72.599410\n",
      "[Epoch 15] train=0.627027 loss=1.369598 time: 78.906116\n",
      "[Epoch 16] train=0.686486 loss=1.176971 time: 68.645279\n",
      "[Epoch 17] train=0.600000 loss=1.458098 time: 69.332837\n",
      "[Epoch 18] train=0.616216 loss=1.479428 time: 63.111423\n",
      "[Epoch 19] train=0.616216 loss=1.846541 time: 69.096201\n",
      "[Epoch 20] train=0.572973 loss=1.738715 time: 65.491584\n",
      "[Epoch 21] train=0.589189 loss=2.150240 time: 63.370385\n",
      "[Epoch 22] train=0.675676 loss=1.716634 time: 62.043841\n",
      "[Epoch 23] train=0.659459 loss=1.591900 time: 62.816021\n",
      "[Epoch 24] train=0.572973 loss=1.882232 time: 60.193215\n",
      "[Epoch 25] train=0.616216 loss=1.718309 time: 58.844510\n",
      "[Epoch 26] train=0.675676 loss=1.214719 time: 57.229208\n",
      "[Epoch 27] train=0.600000 loss=1.882551 time: 56.487940\n",
      "[Epoch 28] train=0.583784 loss=1.678680 time: 56.915152\n",
      "[Epoch 29] train=0.718919 loss=1.764481 time: 55.753312\n",
      "[Epoch 30] train=0.572973 loss=2.613447 time: 56.123664\n",
      "[Epoch 31] train=0.605405 loss=1.944903 time: 55.257062\n",
      "[Epoch 32] train=0.643243 loss=2.047645 time: 53.621161\n",
      "[Epoch 33] train=0.562162 loss=1.527906 time: 52.940503\n",
      "[Epoch 34] train=0.691892 loss=1.440181 time: 52.851444\n",
      "[Epoch 35] train=0.659459 loss=1.130002 time: 52.457442\n",
      "[Epoch 36] train=0.670270 loss=1.118869 time: 53.715557\n",
      "[Epoch 37] train=0.686486 loss=0.982961 time: 53.057044\n",
      "[Epoch 38] train=0.600000 loss=1.300696 time: 53.520440\n",
      "[Epoch 39] train=0.659459 loss=1.209607 time: 52.477656\n",
      "[Epoch 40] train=0.675676 loss=1.287533 time: 51.778097\n",
      "[Epoch 41] train=0.718919 loss=0.911483 time: 52.242127\n",
      "[Epoch 42] train=0.610811 loss=1.066958 time: 52.260007\n",
      "[Epoch 43] train=0.751351 loss=0.844083 time: 51.610182\n",
      "[Epoch 44] train=0.643243 loss=0.779565 time: 52.280994\n",
      "[Epoch 45] train=0.702703 loss=0.739282 time: 51.793812\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        # Extract data and label\n",
    "        data = split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "        label = split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = []\n",
    "            for _, X in enumerate(data):\n",
    "                X = X.reshape((-1,) + X.shape[2:])\n",
    "                pred = net(X)\n",
    "                output.append(pred)\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        train_loss += sum([l.mean().asscalar() for l in loss])\n",
    "        train_metric.update(label, output)\n",
    "\n",
    "        if i == 100:\n",
    "            break\n",
    "\n",
    "    name, acc = train_metric.get()\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([acc])\n",
    "    print('[Epoch %d] train=%f loss=%f time: %f' %\n",
    "        (epoch, acc, train_loss / (i+1), time.time()-tic))\n",
    "\n",
    "# We can plot the metric scores with:\n",
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab685fec1c42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/irteam/dcloud-global-dir/Ahpuh/net.params\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "file_name = \"/home/irteam/dcloud-global-dir/Ahpuh/net.params\"\n",
    "net.save_parameters(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/home/irteam/dcloud-global-dir/Ahpuh/net.params\"\n",
    "\n",
    "new_net = get_model(name='slowfast_4x16_resnet50_kinetics400', nclass=4)\n",
    "new_net.load_parameters(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Decord is required, you can install by `pip install decord --user`         (note that this is unofficial PYPI package).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gluoncv/utils/filesystem.py\u001b[0m in \u001b[0;36mtry_import\u001b[0;34m(package, message, fromlist)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfromlist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfromlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'decord'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-b17497a0b2d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgluoncv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilesystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtry_import_decord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtry_import_decord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/irteam/dcloud-global-dir/Ahpuh/swim_dataset/train/else/_WD4C5G7GUE_000103_000113.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfast_frame_id_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gluoncv/utils/filesystem.py\u001b[0m in \u001b[0;36mtry_import_decord\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0mDecord\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mrequired\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myou\u001b[0m \u001b[0mcan\u001b[0m \u001b[0minstall\u001b[0m \u001b[0mby\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpip\u001b[0m \u001b[0minstall\u001b[0m \u001b[0mdecord\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0muser\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0munofficial\u001b[0m \u001b[0mPYPI\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtry_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'decord'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtry_import_mmcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gluoncv/utils/filesystem.py\u001b[0m in \u001b[0;36mtry_import\u001b[0;34m(package, message, fromlist)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtry_import_cv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Decord is required, you can install by `pip install decord --user`         (note that this is unofficial PYPI package)."
     ]
    }
   ],
   "source": [
    "from gluoncv.utils.filesystem import try_import_decord\n",
    "decord = try_import_decord()\n",
    "\n",
    "vr = decord.VideoReader('/home/irteam/dcloud-global-dir/Ahpuh/swim_dataset/train/else/_WD4C5G7GUE_000103_000113.mp4')\n",
    "fast_frame_id_list = range(0, 64, 2)\n",
    "slow_frame_id_list = range(0, 64, 16)\n",
    "frame_id_list = list(fast_frame_id_list) + list(slow_frame_id_list)\n",
    "video_data = vr.get_batch(frame_id_list).asnumpy()\n",
    "clip_input = [video_data[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "Video data is downloaded and preprocessed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "input_file = '/home/irteam/dcloud-global-dir/Ahpuh/swim_dataset/train/else/_WD4C5G7GUE_000103_000113.mp4'\n",
    "cap = cv2.VideoCapture(input_file)\n",
    "\n",
    "result = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "    \n",
    "    # Resize the frame\n",
    "    print(frame.shape)\n",
    "    resized_frame = cv2.resize(frame, (224, 224))\n",
    "    result.append(resized_frame)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "result = np.array(result)\n",
    "\n",
    "fast_frame_id_list = range(0, 64, 2)\n",
    "slow_frame_id_list = range(0, 64, 16)\n",
    "frame_id_list = list(fast_frame_id_list) + list(slow_frame_id_list)\n",
    "clip_input = [result[vid, :, :, :] for vid, _ in enumerate(frame_id_list)]\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Fix the input video frames size as 256×340 and randomly sample the cropping width and height from\n",
    "    # {256,224,192,168}. After that, resize the cropped regions to 224 × 224.\n",
    "    #video.ShortSideRescale(224),\n",
    "    # Randomly flip the video frames horizontally\n",
    "    #video.VideoRandomHorizontalFlip(),\n",
    "    # Transpose the video frames from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    video.VideoToTensor(),\n",
    "    # Normalize the video frames with mean and standard deviation calculated across all images\n",
    "    video.VideoNormalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# transform_fn = video.VideoGroupValTransform(size=224, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "clip_input = transform_train(clip_input)\n",
    "clip_input = np.stack(clip_input, axis=0)\n",
    "clip_input = clip_input.reshape((-1,) + (36, 3, 224, 224))\n",
    "clip_input = np.transpose(clip_input, (0, 2, 1, 3, 4))\n",
    "print('Video data is downloaded and preprocessed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[2. 0. 1. 3.]\n",
       "<NDArray 4 @cpu(0)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = new_net(nd.array(clip_input))\n",
    "nd.topk(pred, k=4)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n",
      "finish img saving\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "for i in range(36):\n",
    "    img = Image.fromarray(np.transpose(clip_input, (0, 2, 3, 4, 1))[0][i], 'RGB')\n",
    "    # img = Image.fromarray(clip_input[i], 'RGB')\n",
    "    img.save('/home/irteam/didwldn3032-dcloud-dir/didwldn3032/2023-1-SCS4031-Ahpuh/test_imgs/img%s.jpg'%i)\n",
    "    print(\"finish img saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input video clip is classified to be\n",
      "probability 1.000.\n",
      "probability 0.000.\n",
      "probability 0.000.\n"
     ]
    }
   ],
   "source": [
    "label = {0:'diving', 1:'drown', 2:'falldown', 3:'else'}\n",
    "topK = 3\n",
    "ind = nd.topk(pred, k=topK)[0].astype('int')\n",
    "print('The input video clip is classified to be')\n",
    "for i in range(topK):\n",
    "    print('probability %.3f.'%\n",
    "          (nd.softmax(pred)[0][ind[i]].asscalar()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the tiny subset, the accuracy number is quite low.\n",
    "You can `Start Training Now`_ on the full Kinetics400 dataset.\n",
    "\n",
    "### References\n",
    "\n",
    ".. [Wang15] Limin Wang, Yuanjun Xiong, Zhe Wang, and Yu Qiao. \\\n",
    "    \"Towards Good Practices for Very Deep Two-Stream ConvNets.\" \\\n",
    "    arXiv preprint arXiv:1507.02159 (2015).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
