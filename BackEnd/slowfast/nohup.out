[14:50:56] ../src/base.cc:79: cuDNN lib mismatch: linked-against version 8100 != compiled-against version 8101.  Set MXNET_CUDNN_LIB_CHECKING=0 to quiet this warning.
Load 247 training samples.
Start Training
[Epoch 0] train=0.514170 loss=1.351414 time: 120.787664
[Epoch 1] train=0.574899 loss=2.196339 time: 116.255787
[Epoch 2] train=0.599190 loss=2.157228 time: 115.485965
[Epoch 3] train=0.595142 loss=2.049421 time: 114.132174
[Epoch 4] train=0.542510 loss=2.189684 time: 111.098927
[Epoch 5] train=0.595142 loss=2.284592 time: 112.544057
[Epoch 6] train=0.582996 loss=1.470270 time: 108.128345
[Epoch 7] train=0.546559 loss=2.062968 time: 110.984819
[Epoch 8] train=0.708502 loss=1.093137 time: 99.034141
[Epoch 9] train=0.611336 loss=1.588893 time: 96.787050
[Epoch 10] train=0.542510 loss=1.782877 time: 103.126459
[Epoch 11] train=0.599190 loss=1.576620 time: 96.175287
[Epoch 12] train=0.668016 loss=0.920544 time: 91.633799
[Epoch 13] train=0.680162 loss=0.865190 time: 92.435542
[Epoch 14] train=0.700405 loss=0.826768 time: 90.183557
[Epoch 15] train=0.676113 loss=1.082214 time: 92.202905
[Epoch 16] train=0.688259 loss=0.908693 time: 88.862575
[Epoch 17] train=0.708502 loss=1.088444 time: 89.340531
[Epoch 18] train=0.680162 loss=1.279897 time: 89.095261
[Epoch 19] train=0.639676 loss=1.474623 time: 87.021957
[Epoch 20] train=0.659919 loss=0.930920 time: 88.047087
[Epoch 21] train=0.696356 loss=1.359178 time: 88.178615
[Epoch 22] train=0.684211 loss=1.148494 time: 87.487127
[Epoch 23] train=0.639676 loss=1.948946 time: 87.671747
[Epoch 24] train=0.635628 loss=2.008980 time: 85.223539
[Epoch 25] train=0.684211 loss=1.181639 time: 85.340046
[Epoch 26] train=0.582996 loss=2.145523 time: 83.268281
[Epoch 27] train=0.668016 loss=1.952236 time: 82.583566
[Epoch 28] train=0.672065 loss=2.223835 time: 96.588535
[Epoch 29] train=0.663968 loss=2.144984 time: 97.901217
[Epoch 30] train=0.659919 loss=1.840764 time: 96.192611
[Epoch 31] train=0.562753 loss=1.770267 time: 92.160645
[Epoch 32] train=0.574899 loss=1.850110 time: 92.861667
[Epoch 33] train=0.672065 loss=1.189609 time: 92.421023
[Epoch 34] train=0.647773 loss=1.596289 time: 87.555776
[Epoch 35] train=0.659919 loss=1.121396 time: 83.946044
[Epoch 36] train=0.659919 loss=1.350092 time: 84.351318
[Epoch 37] train=0.680162 loss=1.136927 time: 84.478510
[Epoch 38] train=0.663968 loss=0.815339 time: 83.911263
[Epoch 39] train=0.716599 loss=1.023201 time: 83.957690
[Epoch 40] train=0.708502 loss=0.813828 time: 84.345144
[Epoch 41] train=0.769231 loss=0.722172 time: 87.176014
[Epoch 42] train=0.647773 loss=1.161197 time: 92.014867
[Epoch 43] train=0.781377 loss=0.705626 time: 92.806654
[Epoch 44] train=0.744939 loss=0.806555 time: 98.955613
[Epoch 45] train=0.728745 loss=0.929243 time: 93.961806
[Epoch 46] train=0.728745 loss=0.760958 time: 94.469933
[Epoch 47] train=0.748988 loss=0.761258 time: 92.835391
[Epoch 48] train=0.736842 loss=0.715329 time: 83.733116
[Epoch 49] train=0.769231 loss=0.683336 time: 83.588589
[Epoch 50] train=0.728745 loss=0.904849 time: 82.657655
[Epoch 51] train=0.708502 loss=0.822908 time: 85.504753
[Epoch 52] train=0.773279 loss=0.661570 time: 84.714030
[Epoch 53] train=0.773279 loss=0.642426 time: 83.505533
[Epoch 54] train=0.813765 loss=0.575764 time: 84.580532
[Epoch 55] train=0.825911 loss=0.517324 time: 82.771482
[Epoch 56] train=0.813765 loss=0.655287 time: 82.637698
[Epoch 57] train=0.821862 loss=0.524714 time: 82.306909
[Epoch 58] train=0.821862 loss=0.503318 time: 81.403809
[Epoch 59] train=0.813765 loss=0.530676 time: 80.299264
[Epoch 60] train=0.817814 loss=0.550802 time: 80.909184
[Epoch 61] train=0.805668 loss=0.480760 time: 79.991492
[Epoch 62] train=0.805668 loss=0.564293 time: 79.556633
[Epoch 63] train=0.801619 loss=0.601435 time: 79.891827
[Epoch 64] train=0.825911 loss=0.437917 time: 79.267849
[Epoch 65] train=0.825911 loss=0.507487 time: 80.427319
[Epoch 66] train=0.850202 loss=0.555426 time: 80.321836
[Epoch 67] train=0.862348 loss=0.353267 time: 80.529658
[Epoch 68] train=0.874494 loss=0.383013 time: 80.380024
[Epoch 69] train=0.878543 loss=0.359409 time: 80.221224
[Epoch 70] train=0.882591 loss=0.329451 time: 80.358328
[Epoch 71] train=0.882591 loss=0.317998 time: 79.298054
[Epoch 72] train=0.890688 loss=0.315805 time: 80.119142
[Epoch 73] train=0.886640 loss=0.435206 time: 80.022519
[Epoch 74] train=0.866397 loss=0.353494 time: 80.176111
[Epoch 75] train=0.874494 loss=0.319710 time: 80.140743
[Epoch 76] train=0.890688 loss=0.323955 time: 80.142857
[Epoch 77] train=0.890688 loss=0.366791 time: 85.562943
[Epoch 78] train=0.886640 loss=0.363998 time: 90.611508
[Epoch 79] train=0.846154 loss=0.522923 time: 89.993563
[Epoch 80] train=0.862348 loss=0.396477 time: 97.772443
[Epoch 81] train=0.874494 loss=0.413419 time: 100.442732
[Epoch 82] train=0.894737 loss=0.282819 time: 99.651002
[Epoch 83] train=0.878543 loss=0.274687 time: 96.814911
[Epoch 84] train=0.906883 loss=0.291707 time: 90.998098
[Epoch 85] train=0.902834 loss=0.314994 time: 88.412213
[Epoch 86] train=0.919028 loss=0.253801 time: 84.617323
[Epoch 87] train=0.866397 loss=0.403210 time: 81.517507
[Epoch 88] train=0.898785 loss=0.307333 time: 79.923112
[Epoch 89] train=0.910931 loss=0.258340 time: 79.625331
[Epoch 90] train=0.914980 loss=0.233799 time: 81.181615
[Epoch 91] train=0.959514 loss=0.164927 time: 80.214461
[Epoch 92] train=0.939271 loss=0.250146 time: 80.924876
[Epoch 93] train=0.951417 loss=0.155421 time: 80.525493
[Epoch 94] train=0.955466 loss=0.123910 time: 80.200571
[Epoch 95] train=0.935223 loss=0.218677 time: 80.238651
[Epoch 96] train=0.910931 loss=0.254379 time: 79.578315
[Epoch 97] train=0.927126 loss=0.207890 time: 79.538376
[Epoch 98] train=0.939271 loss=0.240572 time: 77.014623
[Epoch 99] train=0.931174 loss=0.230389 time: 81.252655
/home/irteam/anaconda3/envs/mmcv/lib/python3.8/site-packages/gluoncv/__init__.py:40: UserWarning: Both `mxnet==1.9.1` and `torch==1.7.1+cu110` are installed. You might encounter increased GPU memory footprint if both framework are used at the same time.
  warnings.warn(f'Both `mxnet=={mx.__version__}` and `torch=={torch.__version__}` are installed. '
